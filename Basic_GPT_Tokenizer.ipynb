{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRawik2emirUdnXf/Z8iDS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muditbac/nn-learn-notebooks/blob/main/Basic_GPT_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAA6zURK5riX",
        "outputId": "e89e77bc-1150-4c8a-e0dd-57eeef52c184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-18 07:29:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-03-18 07:29:13 (60.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Data for training our tokenizer\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this colab, we will learn about shakespere and through him we will learn about tokenization in LLMs. So let's build a quick bpe tokenizer first.\n",
        "\n",
        "**Quick intro about BPE**: at each iteration it merges most frequent two chracters into a new characters and replaces all of it.\n",
        "\n",
        "Sample Text: aababcbab\n",
        "- find most frequent pair -> ab (3)\n",
        "- replace ab with new char X -> aXXcbX\n",
        "- no repeating elements can be found.\n",
        "\n",
        "BPE for LLMs are done in bytes space and handle for unicode characters"
      ],
      "metadata": {
        "id": "1BbKbYhl6kPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "import math"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DbFFMKgSh8ew"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title unicode and it's chracters\n",
        "# @markdown Unicode is a universal character encoding standard that includes roughly 100,000 characters to represent characters of different languages. Unicode is unbounded because there's no reason to bound it.\n",
        "\n",
        "sample_text = \"यूनिकोड\"\n",
        "\n",
        "print(sample_text+'\\n')\n",
        "\n",
        "for x in sample_text:\n",
        "  print(x, ord(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "ecL_hS0X7uuY",
        "outputId": "6b958d6f-8aa0-4b68-8765-9d9b7f06462e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "यूनिकोड\n",
            "\n",
            "य 2351\n",
            "ू 2370\n",
            "न 2344\n",
            "ि 2367\n",
            "क 2325\n",
            "ो 2379\n",
            "ड 2337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title unicode converted into bytes\n",
        "for x in sample_text.encode('utf-8'):\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "JvkTbiRM76OL",
        "outputId": "48221525-b5b0-4438-ac57-758a1e5b53bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "224\n",
            "164\n",
            "175\n",
            "224\n",
            "165\n",
            "130\n",
            "224\n",
            "164\n",
            "168\n",
            "224\n",
            "164\n",
            "191\n",
            "224\n",
            "164\n",
            "149\n",
            "224\n",
            "165\n",
            "139\n",
            "224\n",
            "164\n",
            "161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic BPE\n",
        "In this appraoch, we apply BPE directly to the complete corpus. After learning ~1000 tokens, we encode the corpus and visualize the tokeinization process"
      ],
      "metadata": {
        "id": "6wz7dbKheUD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('input.txt').read()"
      ],
      "metadata": {
        "id": "exX7DixN8DWa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bpe has two steps first find the most common pair and replace it\n",
        "def get_stats(array):\n",
        "  count = {}\n",
        "  for x,y in zip(array[:-1], array[1:]):\n",
        "    count[(x,y)] = count.get((x,y), 0)+1\n",
        "  return count\n",
        "\n",
        "s = get_stats(\"aababcbab\".encode())\n",
        "sorted(s.items(), key=lambda x: -x[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm36KN-_9EVN",
        "outputId": "27f5f461-0003-4926-8d2f-1a7f4f111f5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((97, 98), 3), ((98, 97), 2), ((97, 97), 1), ((98, 99), 1), ((99, 98), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(original_array, pair, replace_with):\n",
        "  new_array = []\n",
        "  i = 0\n",
        "  while i<len(original_array):\n",
        "    if i<len(original_array)-1 and (original_array[i], original_array[i+1]) == pair:\n",
        "      new_array.append(replace_with)\n",
        "      i = i+2\n",
        "    else:\n",
        "      new_array.append(original_array[i])\n",
        "      i = i+1\n",
        "  return new_array\n",
        "\n",
        "merge([1,2,3,4,3,2,3], (2,3), 9)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew-txSbm-2kr",
        "outputId": "7f86f050-37a9-47fe-e264-56520fbf5f48"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 9, 4, 3, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = 16\n",
        "\n",
        "class Tokenizer():\n",
        "\n",
        "  def train(self, text):\n",
        "    raise NotImplemented\n",
        "\n",
        "  def encode(self, text):\n",
        "    raise NotImplemented\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    raise NotImplemented\n",
        "\n",
        "class BPETokenizer(Tokenizer):\n",
        "\n",
        "  def __init__(self, num_tokens):\n",
        "    self.num_tokens = num_tokens\n",
        "\n",
        "  def train(self, text):\n",
        "    text = list(text.encode('utf-8'))\n",
        "    initial_len = len(text)\n",
        "    self.merge_history = merge_history = {}\n",
        "    for i in range(256, 256+self.num_tokens):\n",
        "      stats = get_stats(text)\n",
        "      most_common_pair = max(stats, key=lambda x: stats[x])\n",
        "      text = merge(text, most_common_pair, i)\n",
        "      merge_history[most_common_pair] = i\n",
        "      print(f\"merged {most_common_pair}: {stats[most_common_pair]} and replaced with {i}\")\n",
        "\n",
        "    final_len = len(text)\n",
        "    print(f\"initial len {initial_len}, final len {final_len}, compressed by factor of {initial_len / final_len}\")\n",
        "\n",
        "  def encode(self, text):\n",
        "    # iteratively keep merging with lowest index, until nothing else can be merged\n",
        "    text = list(text.encode('utf-8'))\n",
        "\n",
        "    while 1:\n",
        "      stats = get_stats(text)\n",
        "      pair_with_lowest_index = min(stats, key=lambda x: self.merge_history.get(x, math.inf))\n",
        "      print(pair_with_lowest_index)\n",
        "    pass\n",
        "\n",
        "tk = BPETokenizer(num_tokens=5000)\n",
        "tk.train(text)"
      ],
      "metadata": {
        "id": "_O_jqZ-8_rnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encode(self, text):\n",
        "  # iteratively keep merging with lowest index, until nothing else can be merged\n",
        "  text = list(text.encode('utf-8'))\n",
        "\n",
        "  while 1:\n",
        "    stats = get_stats(text)\n",
        "    pair_with_lowest_index = min(stats, key=lambda x: self.merge_history.get(x, math.inf))\n",
        "    if pair_with_lowest_index not in self.merge_history:\n",
        "      break\n",
        "    text = merge(text, pair_with_lowest_index, self.merge_history[pair_with_lowest_index])\n",
        "  return text\n",
        "\n",
        "def decode(self, tokens, debug=True):\n",
        "  flattened_tokens_to_chars = {i: [i] for i in range(256)}\n",
        "  token_to_merge = {self.merge_history[i]: i for i in self.merge_history}\n",
        "  final_chars = []\n",
        "\n",
        "  def recursively_fetch(tkn):\n",
        "    if tkn not in flattened_tokens_to_chars:\n",
        "      pair = x,y = token_to_merge[tkn]\n",
        "      flattened_tokens_to_chars[tkn] = recursively_fetch(x) + recursively_fetch(y)\n",
        "    return flattened_tokens_to_chars[tkn]\n",
        "\n",
        "  for x in tokens:\n",
        "    final_chars.extend(recursively_fetch(x))\n",
        "    final_chars.append(124)\n",
        "  return bytearray(final_chars).decode('utf-8', errors='ignore')\n",
        "\n",
        "sample_text = \"Lafeu, consoling the Countess on the death of her husband and departure of her son. (All's Well That Ends Well)\"\n",
        "sample_text = \"Five tribunes to defend their vulgar wisdoms, Of their own choice: one's Junius Brutus,\"\n",
        "sample_text = \"\"\"He--to give fear to use and liberty,\n",
        "Which have for long run by the hideous law,\n",
        "As mice by lions--hath pick'd out an act,\n",
        "Under whose heavy sense your brother's life\n",
        "Falls into forfeit: he arrests him on it;\n",
        "And follows close the rigour of the statute,\n",
        "\"\"\"\n",
        "sample_text = \"we want to break free\"\n",
        "print(encode(tk, sample_text))\n",
        "print(decode(tk, encode(tk, sample_text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbN2b0LtDQjO",
        "outputId": "75e1bce9-d503-4458-8707-d920ad3d24eb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[538, 119, 906, 283, 2411, 413, 309, 101]\n",
            "we |w|ant |to |break| f|re|e|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Outputs\n",
        "```\n",
        "[70, 561, 3467, 3371, 4223, 1950, 118, 465, 1485, 288, 3802, 377, 702, 2075, 2446, 2835, 1281, 541, 274, 691, 74, 347, 3447, 66, 414, 116, 441, 44]\n",
        "F|ive |tribun|es to |defend |their |v|ul|gar| w|isdom|s, |Of| their |own |cho|ice|: |on|e's |J|un|ius |B|ru|t|us|,|\n",
        "```\n",
        "\n",
        "```\n",
        "[72, 101, 530, 283, 803, 895, 399, 801, 284, 2679, 1491, 1266, 3291, 619, 894, 32, 1330, 3283, 104, 440, 2428, 1157, 1530, 109, 1167, 471, 306, 274, 115, 530, 570, 367, 657, 384, 600, 2915, 504, 277, 1043, 580, 3146, 3336, 2734, 379, 332, 2012, 325, 622, 394, 70, 493, 2114, 283, 300, 2403, 316, 2364, 272, 449, 900, 1675, 960, 316, 589, 1751, 259, 1702, 379, 295, 335, 103, 323, 1828, 2686, 491, 452]\n",
        "H|e|--|to |give |fear| to |use |and |liber|ty|,\n",
        "Which| have |for |long| |run| by the |h|id|eous |law|,\n",
        "As |m|ice |by |li|on|s|--|hath| p|ick|'d |out |an a|ct|,\n",
        "|Un|der| whose |heavy |sen|se |your| brother|'s |lif|e\n",
        "|F|all|s in|to |for|fe|it|: he |ar|res|ts |him |on |it|;\n",
        "And |follow|s |clo|se |the |ri|g|our| of the |stat|ut|e,\n",
        "|\n",
        "\n",
        "```\n",
        "\n",
        "The problem with above BPE\n",
        "- it creates tokens across multiple words. For example in the above example tokens like `es to ` and `s in` are split across chracters, which might increase noise in out training data\n",
        "- number and math symbols in the corpus will be treated as single token which is not a good representation. instead, single digit and symbol should be one token"
      ],
      "metadata": {
        "id": "3I7JV5TktGoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = encode(tk, text)"
      ],
      "metadata": {
        "id": "bQGzMtb1E8CL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## what percentage of all the tokens are used in the final output?\n",
        "\n",
        "BPE merge two tokens to creates a new tokens. I had imageined that this will lead to lot's of smallers tokens not being used because the smaller tokens will be merged to created larger tokens during the encoding process. The below cells calculates all the unique set of tokens and it's ratio after training."
      ],
      "metadata": {
        "id": "APNwtUI6dwPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(all_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoF00HdlIcXo",
        "outputId": "52672337-9e6b-4e56-a74d-ca27cdc926fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1035"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(all_tokens).difference(range(256)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVBcJdZ8IWsb",
        "outputId": "a3147247-bdab-4ed9-97e4-54fdeb5b5e39"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "971"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(all_tokens).difference(range(256))) / (1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCHWnJc-ZCQG",
        "outputId": "5dbd84cf-ab5a-48ed-a6f8-6f865ed037d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.971"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE with Regex Split (GPT-2)\n",
        "- TBD"
      ],
      "metadata": {
        "id": "nQHuzF5le3uh"
      }
    }
  ]
}